{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv('../.env_api')\n",
    "from rich.pretty import pprint\n",
    "from operator import add\n",
    "from litellm import completion\n",
    "\n",
    "def ppprint(obj):\n",
    "    pprint(obj, indent_guides=False)\n",
    "\n",
    "os.environ[\"HTTP_PROXY\"] = \"http://localhost:9090\"\n",
    "os.environ[\"HTTPS_PROXY\"] = \"http://localhost:9090\"\n",
    "os.environ[\"REQUESTS_CA_BUNDLE\"] = \"/Users/tomek/Library/Application Support/com.proxyman.NSProxy-setapp/app-data/proxyman-ca.pem\"\n",
    "os.environ[\"SSL_CERT_FILE\"] = \"/Users/tomek/Library/Application Support/com.proxyman.NSProxy-setapp/app-data/proxyman-ca.pem\"\n",
    "from langsmith import Client\n",
    "import os\n",
    "\n",
    "from typing import List, Annotated, Any, Dict, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import instructor\n",
    "from openai import OpenAI\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root / \"src\"))\n",
    "\n",
    "\n",
    "from api.rag.utils.utils import lc_messages_to_regular_messages\n",
    "from api.rag.utils.utils import prompt_template_config\n",
    "from api.core.config import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinator agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCPToolCall(BaseModel):\n",
    "    name: str\n",
    "    arguments: dict\n",
    "    server: str\n",
    "\n",
    "class ToolCall(BaseModel):\n",
    "    name: str\n",
    "    arguments: dict\n",
    "\n",
    "class RAGUsedContext(BaseModel):\n",
    "    id: str\n",
    "    description: str\n",
    "    \n",
    "class Delegation(BaseModel):\n",
    "    agent: str\n",
    "    task: str = Field(default=\"\")\n",
    "\n",
    "class CoordinatorAgentResponse(BaseModel):\n",
    "    next_agent: str\n",
    "    plan: list[Delegation]\n",
    "    final_answer: bool = Field(default=False)\n",
    "    answer: str\n",
    "\n",
    "class State(BaseModel):\n",
    "    messages: Annotated[List[Any], add] = []\n",
    "    answer: str = \"\"\n",
    "\n",
    "    coordinator_iteration: int = Field(default=0)\n",
    "    product_qa_iteration: int = Field(default=0)\n",
    "    shopping_cart_iteration: int = Field(default=0)\n",
    "\n",
    "    coordinator_final_answer: bool = Field(default=False)\n",
    "    product_qa_final_answer: bool = Field(default=False)\n",
    "    shopping_cart_final_answer: bool = Field(default=False)\n",
    "\n",
    "    product_qa_available_tools: List[Dict[str, Any]] = []\n",
    "    shopping_cart_available_tools: List[Dict[str, Any]] = []\n",
    "\n",
    "    tool_calls: Optional[List[ToolCall]] = Field(default_factory=list)\n",
    "    mcp_tool_calls: Optional[List[MCPToolCall]] = Field(default_factory=list)\n",
    "    retrieved_context: List[RAGUsedContext] = Field(default_factory=list)\n",
    "    \n",
    "    user_id: str = \"\"\n",
    "    cart_id: str = \"\"\n",
    "\n",
    "    next_agent: str = \"\"\n",
    "    plan: list[Delegation] = Field(default_factory=list)\n",
    "\n",
    "    trace_id: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinator_agent_node(state, models = ['gpt-4.1', 'groq/llama-3.3-70b-versatile']) -> dict:\n",
    "    template = prompt_template_config('../' + config.PROMPT_TEMPLATE_PATH, 'coordinator_agent')\n",
    "    \n",
    "    prompt = template.render()\n",
    "\n",
    "    messages = state.messages\n",
    "\n",
    "    conversation = []\n",
    "\n",
    "    for msg in messages:\n",
    "        conversation.append(lc_messages_to_regular_messages(msg))\n",
    "\n",
    "    client = instructor.from_litellm(completion)\n",
    "\n",
    "    for model in models:\n",
    "        try:\n",
    "            response, raw_response = client.chat.completions.create_with_completion(\n",
    "                model=model,\n",
    "                response_model=CoordinatorAgentResponse,\n",
    "                messages=[{\"role\": \"system\", \"content\": prompt}, *conversation],\n",
    "                temperature=0,\n",
    "            )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error with model {model}: {e}\")\n",
    "\n",
    "    return {\n",
    "        # \"messages\": ai_message,\n",
    "        \"next_agent\": response.next_agent,\n",
    "        \"plan\": response.plan,\n",
    "        \"coordinator_final_answer\": response.final_answer,\n",
    "        \"coordinator_iteration\": state.coordinator_iteration + 1,\n",
    "        \"answer\": response.answer,\n",
    "        \"trace_id\": ''\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = State(messages=[    \n",
    "    {\"role\": \"user\", \"content\": \"What is the weather in Tokyo?\"}\n",
    "])\n",
    "\n",
    "#answer = coordinator_agent_node(initial_state, models=['groq/llama-3.3-70b-versatile'])\n",
    "answer = coordinator_agent_node(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'next_agent'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'plan'</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'coordinator_final_answer'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'coordinator_iteration'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'answer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"I'm here to assist you with shopping-related questions, such as finding products, checking specifications, or managing your shopping cart. I don't have access to real-time weather information. If you have any questions about products or shopping, feel free to ask!\"</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'trace_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'next_agent'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'plan'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'coordinator_final_answer'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'coordinator_iteration'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'answer'\u001b[0m: \u001b[32m\"I'm here to assist you with shopping-related questions, such as finding products, checking specifications, or managing your shopping cart. I don't have access to real-time weather information. If you have any questions about products or shopping, feel free to ask!\"\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'trace_id'\u001b[0m: \u001b[32m''\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
